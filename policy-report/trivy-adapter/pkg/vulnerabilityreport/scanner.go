package vulnerabilityreport

import (
	"context"
	"fmt"

	policyreport "sigs.k8s.io/wg-policy-prototypes/policy-report/api/v1alpha2"
	"github.com/aquasecurity/starboard/pkg/starboard"
	"github.com/aquasecurity/starboard/pkg/docker"
	"github.com/aquasecurity/starboard/pkg/kube"
	"github.com/aquasecurity/starboard/pkg/runner"
	"github.com/kubernetes-sigs/wg-policy-prototypes/policy-report/trivy-adapter/pkg/imgvuln"
	batchv1 "k8s.io/api/batch/v1"
	corev1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/runtime"
	"k8s.io/client-go/kubernetes"
	"k8s.io/klog"
	"k8s.io/utils/pointer"
	"sigs.k8s.io/controller-runtime/pkg/client"
)

// Scanner is a template for running static vulnerability scanners that implement
// the Plugin interface.
type Scanner struct {
	scheme         *runtime.Scheme
	clientset      kubernetes.Interface
	plugin         Plugin
	pluginContext  imgvuln.PluginContext
	objectResolver *kube.ObjectResolver
	logsReader     kube.LogsReader
	config         imgvuln.ConfigData
	opts           kube.ScannerOpts
	secretsReader  kube.SecretsReader
}

// NewScanner constructs a new static vulnerability Scanner with the specified
// Plugin that knows how to perform the actual scanning,
// which is performed by running a Kubernetes job, and knows how to convert logs
// to instances of PolicyReport.
func NewScanner(
	clientset kubernetes.Interface,
	client client.Client,
	plugin Plugin,
	pluginContext imgvuln.PluginContext,
	config imgvuln.ConfigData,
	opts kube.ScannerOpts,
) *Scanner {
	return &Scanner{
		scheme:         client.Scheme(),
		clientset:      clientset,
		opts:           opts,
		plugin:         plugin,
		pluginContext:  pluginContext,
		objectResolver: &kube.ObjectResolver{Client: client},
		logsReader:     kube.NewLogsReader(clientset),
		config:         config,
		secretsReader:  kube.NewSecretsReader(client),
	}
}

func (s *Scanner) ScanPolicy(ctx context.Context, workload kube.Object) ([]policyreport.PolicyReport, error) {

	klog.V(3).Infof("Getting Pod template for workload: %v", workload)

	owner, err := s.objectResolver.GetObjectFromPartialObject(ctx, workload)
	if err != nil {
		return nil, fmt.Errorf("resolving object: %w", err)
	}
	spec, err := kube.GetPodSpec(owner)
	if err != nil {
		return nil, fmt.Errorf("getting Pod template: %w", err)
	}

	klog.V(3).Infof("Scanning with options: %+v", s.opts)

	credentials, err := s.getCredentials(ctx, spec, workload.Namespace)
	if err != nil {
		return nil, err
	}

	job, secrets, err := s.prepareScanJob(owner, spec, credentials)
	if err != nil {
		return nil, fmt.Errorf("preparing scan job: %w", err)
	}

	err = runner.New().Run(ctx, kube.NewRunnableJob(s.scheme, s.clientset, job, secrets...))
	if err != nil {
		return nil, fmt.Errorf("running scan job: %w", err)
	}

	defer func() {
		if !s.opts.DeleteScanJob {
			klog.V(3).Infof("Skipping scan job deletion: %s/%s", job.Namespace, job.Name)
			return
		}
		klog.V(3).Infof("Deleting scan job: %s/%s", job.Namespace, job.Name)
		background := metav1.DeletePropagationBackground
		_ = s.clientset.BatchV1().Jobs(job.Namespace).Delete(ctx, job.Name, metav1.DeleteOptions{
			PropagationPolicy: &background,
		})
	}()

	klog.V(3).Infof("Scan job completed: %s/%s", job.Namespace, job.Name)

	return s.getPolicyReportsByScanJob(ctx, job, owner)
}


func (s *Scanner) getCredentials(ctx context.Context, spec corev1.PodSpec, ns string) (map[string]docker.Auth, error) {
	imagePullSecrets, err := s.secretsReader.ListImagePullSecretsByPodSpec(ctx, spec, ns)
	if err != nil {
		return nil, err
	}
	return kube.MapContainerNamesToDockerAuths(kube.GetContainerImagesFromPodSpec(spec), imagePullSecrets)
}

func (s *Scanner) prepareScanJob(workload client.Object, spec corev1.PodSpec, credentials map[string]docker.Auth) (*batchv1.Job, []*corev1.Secret, error) {
	templateSpec, secrets, err := s.plugin.GetScanJobSpec(s.pluginContext, spec, credentials)
	if err != nil {
		return nil, nil, err
	}

	scanJobTolerations, err := s.config.GetScanJobTolerations()
	if err != nil {
		return nil, nil, err
	}
	templateSpec.Tolerations = append(templateSpec.Tolerations, scanJobTolerations...)

	templateSpec.ServiceAccountName = imgvuln.ServiceAccountName

	containerImagesAsJSON, err := kube.GetContainerImagesFromPodSpec(spec).AsJSON()
	if err != nil {
		return nil, nil, err
	}

	scanJobAnnotations, err := s.config.GetScanJobAnnotations()
	if err != nil {
		return nil, nil, err
	}

	podSpecHash := kube.ComputeHash(spec)

	labels := map[string]string{
		imgvuln.LabelResourceKind:               workload.GetObjectKind().GroupVersionKind().Kind,
		imgvuln.LabelResourceName:               workload.GetName(),
		imgvuln.LabelResourceNamespace:          workload.GetNamespace(),
		imgvuln.LabelPodSpecHash:                podSpecHash,
		imgvuln.LabelK8SAppManagedBy:            imgvuln.Appimgvuln,
		imgvuln.LabelVulnerabilityReportScanner: "true",
	}

	return &batchv1.Job{
		ObjectMeta: metav1.ObjectMeta{
			Name:      GetScanJobName(workload),
			Namespace: imgvuln.NamespaceName,
			Labels:    labels,
			Annotations: map[string]string{
				starboard.AnnotationContainerImages: containerImagesAsJSON,
			},
		},
		Spec: batchv1.JobSpec{
			BackoffLimit:          pointer.Int32Ptr(0),
			Completions:           pointer.Int32Ptr(1),
			ActiveDeadlineSeconds: kube.GetActiveDeadlineSeconds(s.opts.ScanJobTimeout),
			Template: corev1.PodTemplateSpec{
				ObjectMeta: metav1.ObjectMeta{
					Labels:      labels,
					Annotations: scanJobAnnotations,
				},
				Spec: templateSpec,
			},
		},
	}, secrets, nil
}

func (s *Scanner) getPolicyReportsByScanJob(ctx context.Context, job *batchv1.Job, owner client.Object) ([]policyreport.PolicyReport, error) {

	var results []policyreport.PolicyReport

	containerImages, err := kube.GetContainerImagesFromJob(job)
	if err != nil {
		return nil, fmt.Errorf("getting container images: %w", err)
	}

	podSpecHash, ok := job.Labels[imgvuln.LabelPodSpecHash]
	if !ok {
		return nil, fmt.Errorf("expected label %s not set", imgvuln.LabelPodSpecHash)
	}

	for containerName, containerImage := range containerImages {
		klog.V(3).Infof("Getting logs for %s container in job: %s/%s", containerName, job.Namespace, job.Name)
		logsStream, err := s.logsReader.GetLogsByJobAndContainerName(ctx, job, containerName)
		if err != nil {
			return nil, err
		}

		result, err := s.plugin.ParsePolicyReportData(s.pluginContext, containerImage, logsStream)
		if err != nil {
			return nil, err
		}

		_ = logsStream.Close()

		report, err := NewReportBuilderPolicy(s.scheme).
			ControllerPolicy(owner).
			ContainerPolicy(containerName).
			DataPolicy(result).
			PodSpecHashPolicy(podSpecHash).
			GetPolicy()
		if err != nil {
			return nil, err
		}

		results = append(results, report)

	}
	return results, nil
}